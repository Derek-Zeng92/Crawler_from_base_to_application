01_浏览器
    一个网页的加载过程:
        输入url. 浏览器如何展示出一个完整的页面的.
        1. 浏览器发送请求后
           服务器组织好一段html代码, 反馈给浏览器
           浏览器对HTML进行执行.
           执行之后展示给用户看

        2. 异步加载.
            第一次请求实际上只拿到了html的片段.
            随着时间的推移或者某个事件的产生.
            触发一个新的请求. 会返回给你一些数据
            通过网页上的代码把数据渲染到页面当中.

        区别:你要的东西在不在页面源代码

    浏览器抓包工具(看network)
        # pip install brotli 避免br压缩的内容无法解开

02. 如何用python发送这个请求
    处理http1.1, requests.
        1. get请求
            url = "https://www.baidu.com/s?ie=UTF-8&wd=%E5%91%A8%E6%9D%B0%E4%BC%A6"
            requests.get(url, headers={})

            请注意.
            所有的Query String Parameters 中的内容
            自动的都要添加到url后面
            处理方案:
                1. 你可以直接把url拷贝过来(url中的参数不需要逆向的情况下), 直接url就有参数了

                2. 可以复制url的时候. 不带着?后面的东西.
                    所有的参数通过字典进行组装
                    url = "https://www.baidu.com/s"
                    # 计算某个参数
                    sign = xxxx
                    dic = {
                        参数
                        "xxx": sign
                    }
                    在发送请求时. requests.get(url, params=dic)
                    在requests内部会自动进行参数的拼接
            Query String Parameters 中的内容都要进入到url里面去.

        2. post请求
            根据content-type的不同而使用不同的方案.
            1. application/x-www-form-urlencoded; charset=UTF-8
                此时, payload中 看到Form Data
                此时, 本次请求其实和浏览器中的form表单提交是一回事儿.
                我们只需要准备好字典类型的参数
                dic = {
                    form data中的各种参数
                }

                requests.post(url, data=dic)  # 请求是直接给data参数即可

            2. application/json;charset=UTF-8
                当payload中出现Request Payload
               此时说明, 请求体是一个json字符串
               import json

               dic = {
                    浏览器上看到的参数
               }
               # 方案一.
               # 坑: 默认情况下 requests的 请求头里面的content-type是按照上面的逻辑进行编写的
               # 解决方案. 给出自己请求头.在自己的请求头中, 给出content-type: application/json;charset=UTF-8
               requests.post(url, data=json.dumps(dic),  headers={需要给content-type})

               # 方案二.  坑. 如果服务器检测参数的空格问题. 它无法应对.
               requests.post(url, json=dic, headers={请求头就不用给了content-type})

    处理http2.0, httpx  -> pip install httpx
        import httpx

        client = httpx.Client(http2=True)
        client.get()
        client.post()

03. 数据解析
    re,
        作用. 从一个字符串里提取你需要的字符串.
        从javascript的代码里面提取到一些内容 -> re最好用
        其次是字符串基本操作. replace, split, strip, 字符串切片
    xpath,
        从html中提取你需要的节点.属性.文本内容
        /  根节点
        //  全局搜索
        ./  从当前节点出发
        //div[@属性=值]/text()
        //div[@属性=值]/@属性

        lxml中etree模块的xpath功能. 返回的东西一定是列表

        技巧. 尽量往一条一条数据上靠

    bs4,
        find(标签, attrs={属性:值})
        find_all(标签, attrs={属性:值})
        select_one(css选择器)
        select(css选择器)

        bs4的使用场景: 当你遇到非标准html的时候(自定义xml, svg)

    json,
        处理返回的json的第一步:
            把json字符串处理成python的字典
            1. json模块, json.loads()
            2. response.json()

            不论以上那种方案. 永远永远要记住. 一定把程序跑通了. 确定好, 服务器返回的是json.
            后面如果谁. response.text->html  response.json()   拉出去枪毙.
        剩下的. 就按照汪峰的处理逻辑.

    jsonp,
        看起来: xxxx(大号json) <- 字符串
        obj = re.compile(r"\((?P<name>.*)\)")

        s = obj.search(resp.text).group("name")
        dic = json.loads(s)
        继续汪峰的逻辑

04. 关于请求头(常规)
    1. User-Agent: 表示发送请求的设备. 复制浏览器上的UA.
    2. Referer: 防盗链.
        http协议中. 表示. 当前这个请求的来源.
        注意, 在抓包中, get请求. 双击url. 看是否能获取到正确的结果
        如果能, 基本上不需要referer
        如果不能. 大概率是需要referer

    3. cookie: 为了保持服务器和客户端的状态
        cookie的本质是服务器在浏览器端保存的一个字符串.
        这个字符串会存在于浏览器端, 每次发送请求的时候.浏览器会自动携带cookie访问服务器

        服务器那边会根据访问时携带的cookie信息. 来查找当前浏览器(设备)在服务器上存储的一些信息.
        有助于服务器辨别客户端的用户状态.

        cookie. 会话.保持客户端和服务器端的状态.

        cookie是怎么来的:
            1. 服务器直接返回的响应头(set-cookie)
                浏览器会自动的把set-cookie中的内容自动保存在浏览器中. 后续的请求. 自动携带该值.
            2. 在浏览器执行一些脚本设置的cookie信息
                javascript可以动态在浏览器设置cookie信息.

        如果你碰到的网站cookie的值.每次请求都不一样.
            1. 如果cookie是从响应头的set-cookie来的.
                此时建议, 使用session来保持住cookie状态.
                requests的session可以自动帮你完成set-cookie的设置.
            2. 如果cookie是js动态生成的. 每次都不一样的
                你需要先想办法计算出来cookie的值.

            建议使用下面的逻辑进行设置
            session = requests.session()
            session.headers['ua'] = ua的值
            session.headers['referer'] = re的值
            session.cookies[a] = a的值
            session.cookies[b] = a的值
            session.cookies[e] = 你去计算
            session.get(url)
            session.cookies[e] = 你去计算
            session.get(url)

        如果你要爬的东西. 有可能会被封号.
            建议. 走完登陆流程. 多账号操作. 分开进行访问.
            10个账号
            cookies = [十个账号的cookie信息]
            random

        如果不涉及逆向. 你可以直接复制浏览器上的cookie做测试.

05. 数据存储
    1. csv, excel
        1. 数据量小.
        2. csv和其他分析工具
        本质是文本. 默认情况下. csv以逗号做分隔.
        csv文件默认用excel能打开. 但是,大多数同学的机器打开是乱码
        不要硬用xlwt openpyxl, 用pandas转存一下csv变成xlsx

        写入csv. 按照纯文本的逻辑写.

    2. 数据库
        1. 增加数据
            insert into 表(字段1, 字段2.....) values(值1, 值2 ....)
        2. 修改数据
            update 表 set 字段 = 值, 字段 = 值  where 条件
        3. 删除数据(几乎不用)
            delete from 表 where 条件
        4. 查询
            select distint *|列|count(), sum(), avg() max(), min() from 表 where 条件 group by order by limit

        # python程序操纵mysql
        pymysql
        import pymysql
        import traceback
        try:
            conn =  pymysq.connect(host="", port=3306, user="", password="")
            cursor = conn.cursor()
            novel_name = "斩风"
            novel_author = "樵夫"
            novel_read_count = 18888
            sql = "insert into 表(name, author, read_count) values(%s, %s, %s)"   # 这里的%s不是为了格式化字符串, 占位用的.
            cursor.execute(sql, (novel_name, novel_author, novel_read_count))
            # 先把sql加载给mysql数据库
            # 然后再把这个三个值. 传递给mysql数据库
            # 为了避免sql注入.
            conn.commit()
        except Exception as e:
            conn.rollback()
            print("报错了", e)  # 这样打印是看不到行号的.
            print(traceback.format_exc())  # 能看到报错的行号信息

    3. mongo
        mongo注意一个点. 操作很多是过时的.
        mongo['集合'].insert_one()
        mongo['集合'].insert_many()

    4. 非文本文件(图片, mp3, mp4, zip, pdf, doc, xlsx)

06. 异步
    加快爬取速度(不提倡)
    一般网站单机就可以了.

07. scrapy
    协程. 不是python的async.
    得到程序设计思路.

    scrapy在爬取相同的url的时候. 默认会自动进行过滤.
    请问各位. 是否清楚. scrapy是如何过滤重复url的?
    优秀的调度器.

08. 面向对象
    函数编程. 不搞大项目. 只做小单. 用函数编程足够了.

    1. 发送请求
    2. 对数据进行解析
    3. 对数据进行存储
    核心思维: 按照一个固定的顺序去编写程序. 一步一步的去写代码. 面向过程编程. 一切以流程为核心
    优点: 所见即所得. 思路非常清晰.

    面向对象的核心思维: 一切以对象为中心. 围绕着对象去开发各种功能
    喝可乐:
    1. 起身.
    2. 走到冰箱门口
    3. 打开冰箱
    4. 拿出可乐
    5. 开喝

    换成面向对象:
       让对象帮你去冰箱拿可乐. "你"就不需要亲力亲为了. 所有的任务叫给对象来完成.

    想让对象帮你干活. 前提是. 你得有对象.
    在程序的世界里. 你是上帝. 你是这个世界的主宰. 你想要什么就可以去创造什么?

    在python的世界里怎们搞出来对象.
    你需要先写类 -> 描述出来你创建的对象能做什么?

    你要造人. 你得先规划好. 人能干什么...

    人规划好了。造人。 用人去干活
